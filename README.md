# NNDL-course-captioning

> ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ è¯¾ç¨‹é¡¹ç›®ï¼šåŸºäº ViT + Transformer çš„æœè£…å›¾åƒæè¿°ç”Ÿæˆ

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª**å›¾åƒæè¿°ç”Ÿæˆ (Image Captioning)** ç³»ç»Ÿï¼Œè¾“å…¥ä¸€å¼ æœè£…å›¾ç‰‡ï¼Œè‡ªåŠ¨ç”Ÿæˆæè¿°å›¾ä¸­äººç‰©ç©¿ç€çš„æ–‡æœ¬ã€‚

**æ¨¡å‹æ¶æ„ï¼š**
```
å›¾åƒ (224Ã—224) â†’ ViT Encoder (é¢„è®­ç»ƒ) â†’ å›¾åƒç‰¹å¾ â†’ Transformer Decoder â†’ æ–‡æœ¬æè¿°
```

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
è¾“å…¥: ä¸€å¼ å¥³æ€§ç©¿ç€æ¯›è¡£çš„å›¾ç‰‡
è¾“å‡º: "the sweater this lady wears has long sleeves , its fabric is cotton , and it has pure color patterns ."
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
NNDL-course-captioning/
â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train.json              # è®­ç»ƒé›† (34035 æ ·æœ¬)
â”‚   â”œâ”€â”€ val.json                # éªŒè¯é›† (4254 æ ·æœ¬)
â”‚   â”œâ”€â”€ test.json               # æµ‹è¯•é›† (4255 æ ·æœ¬)
â”‚   â”œâ”€â”€ vocab.json              # è¯è¡¨ (109 tokens)
â”‚   â”œâ”€â”€ captions.json           # åŸå§‹æè¿°æ•°æ®
â”‚   â””â”€â”€ preprocess.py           # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚
â”œâ”€â”€ models/                     # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ vit_encoder_decoder.py  # ä¸»æ¨¡å‹ï¼šViT Encoder + Transformer Decoder
â”‚   â””â”€â”€ vitbackbone.py          # ViT éª¨å¹²ç½‘ç»œ
â”‚
â”œâ”€â”€ trains/                     # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ task6.py                # è®­ç»ƒå…¥å£
â”‚
â”œâ”€â”€ eval/                       # è¯„æµ‹æ¨¡å— â­
â”‚   â”œâ”€â”€ __init__.py             # æ¨¡å—å…¥å£
â”‚   â”œâ”€â”€ stage1_predict.py       # Stage 1: æ¨ç†é¢„æµ‹ + Loss è®¡ç®—
â”‚   â”œâ”€â”€ stage2_metrics.py       # Stage 2: æŒ‡æ ‡è®¡ç®— (METEOR/ROUGE/CIDEr/SPICE)
â”‚   â””â”€â”€ evaluate.py             # ç»Ÿä¸€è¯„æµ‹å…¥å£
â”‚
â”œâ”€â”€ tools/                      # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ dataset.py              # PyTorch Dataset å®šä¹‰
â”‚   â”œâ”€â”€ functions.py            # collate_fn ç­‰å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ token2id.py             # token â†’ id è½¬æ¢
â”‚   â””â”€â”€ id2token.py             # id â†’ token è½¬æ¢
â”‚
â”œâ”€â”€ inference/                  # æ¨ç†æ¨¡å— â­
â”‚   â””â”€â”€ infer.py                # ImageCaptioner æ¨ç†ç±»
â”‚
â”œâ”€â”€ experiments/                # å®éªŒåˆ†æ
â”‚   â””â”€â”€ analysis1.ipynb         # åˆ†æ notebook
â”‚
â””â”€â”€ outputs/                    # è¾“å‡ºç›®å½• (è®­ç»ƒæ—¶ç”Ÿæˆ)
    â”œâ”€â”€ ckpts/                  # æ¨¡å‹æ£€æŸ¥ç‚¹
    â”œâ”€â”€ test_results/           # æµ‹è¯•ç»“æœ
    â””â”€â”€ eval_results/           # è¯„æµ‹ç»“æœ
```

---

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### 1. æ•°æ®é›† JSON (`train.json` / `val.json` / `test.json`)

æ¯æ¡æ•°æ®åŒ…å«å›¾ç‰‡è·¯å¾„ã€token ID åºåˆ—å’Œåºåˆ—é•¿åº¦ï¼š

```json
{
  "img": "WOMEN-Jackets_Coats-id_00007765-03_2_side.jpg",
  "cap_ids": [1, 3, 35, 30, 99, 32, 15, 8, 21, ..., 2],
  "length": 33
}
```

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `img` | å›¾ç‰‡æ–‡ä»¶å |
| `cap_ids` | token ID åºåˆ—ï¼Œä»¥ `<START>=1` å¼€å¤´ï¼Œ`<END>=2` ç»“å°¾ |
| `length` | åºåˆ—é•¿åº¦ (å« START å’Œ END) |

### 2. è¯è¡¨ JSON (`vocab.json`)

åŒ…å« 109 ä¸ª tokensï¼Œæ¶µç›–æœè£…ç›¸å…³è¯æ±‡ï¼š

```json
{
  "token2id": {
    "<PAD>": 0,
    "<START>": 1,
    "<END>": 2,
    "the": 3,
    "sweater": 35,
    "cotton": 12,
    ...
  },
  "id2token": {
    "0": "<PAD>",
    "1": "<START>",
    "2": "<END>",
    "3": "the",
    ...
  },
  "freq": {
    "the": 121842,
    "is": 118379,
    ...
  }
}
```

**ç‰¹æ®Š Tokenï¼š**
| Token | ID | è¯´æ˜ |
|-------|-----|------|
| `<PAD>` | 0 | å¡«å……ç¬¦ |
| `<START>` | 1 | åºåˆ—å¼€å§‹ |
| `<END>` | 2 | åºåˆ—ç»“æŸ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

```bash
pip install torch torchvision tqdm
pip install pycocoevalcap  # è¯„æµ‹æŒ‡æ ‡ (å¯é€‰)
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
# ä¿®æ”¹ trains/task6.py ä¸­çš„ image_root ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
python trains/task6.py
```

**ä¸»è¦è¶…å‚æ•°ï¼š**
| å‚æ•° | å€¼ |
|------|-----|
| epochs | 20 |
| batch_size | 32 |
| learning_rate | 1e-4 |
| d_model | 512 |
| n_heads | 8 |
| num_layers | 4 |

### 3. è¯„æµ‹æ¨¡å‹

**æ–¹å¼ä¸€ï¼šPython ä»£ç **
```python
from eval.evaluate import run_full_evaluation
from models.vit_encoder_decoder import ImageCaptionModel
import torch

# åŠ è½½æ¨¡å‹
model = ImageCaptionModel(vocab_size=109)
model.load_state_dict(torch.load("outputs/ckpts/epoch19.pth"))

# ä¸€é”®è¯„æµ‹
results = run_full_evaluation(
    model=model,
    data_json="data/val.json",
    image_root="/your/image/path",  # ğŸ‘ˆ ä¿®æ”¹ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
    output_dir="outputs/eval_results",
)
```

**æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œ**
```bash
python -m eval.evaluate \
    --checkpoint outputs/ckpts/epoch19.pth \
    --data_json data/val.json \
    --image_root /your/image/path \
    --output_dir outputs/eval_results
```

---

## ğŸ“ˆ è¯„æµ‹ä½“ç³»

è¯„æµ‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

### Stage 1: æ¨ç†é¢„æµ‹ (`stage1_predict.py`)

- è¾“å…¥ï¼šæ¨¡å‹ + æ•°æ®é›† + å›¾ç‰‡è·¯å¾„
- è¾“å‡ºï¼šæ¯ä¸ªæ ·æœ¬çš„ gt_textã€pred_textã€loss

```json
{
  "img": "xxx.jpg",
  "gt_ids": [3, 19, 20, ...],
  "gt_text": "the tank top this female wears ...",
  "pred_ids": [3, 35, 30, ...],
  "pred_text": "the sweater this ...",
  "loss": 0.123456
}
```

### Stage 2: æŒ‡æ ‡è®¡ç®— (`stage2_metrics.py`)

- è¾“å…¥ï¼šStage 1 çš„è¾“å‡º
- è¾“å‡ºï¼šæ¯ä¸ªæ ·æœ¬çš„å››ä¸ªæŒ‡æ ‡ + æ•´ä½“ç»Ÿè®¡

```json
{
  "summary": {
    "total_samples": 4254,
    "avg_loss": 0.5234,
    "avg_metrics": {
      "METEOR": 0.3521,
      "ROUGE_L": 0.4123,
      "CIDEr": 1.2345,
      "SPICE": 0.2134
    }
  },
  "samples": [...]
}
```

**è¯„æµ‹æŒ‡æ ‡ï¼š**
| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| METEOR | è€ƒè™‘åŒä¹‰è¯å’Œè¯å½¢å˜åŒ–çš„åŒ¹é… |
| ROUGE-L | æœ€é•¿å…¬å…±å­åºåˆ— |
| CIDEr-D | åŸºäº TF-IDF çš„å…±è¯†åº¦é‡ |
| SPICE | åŸºäºåœºæ™¯å›¾çš„è¯­ä¹‰åŒ¹é… (éœ€ Java) |

---

## ğŸ”® æ¨ç†ä½¿ç”¨

### æ–¹å¼ä¸€ï¼šPython ä»£ç 

```python
from inference.infer import ImageCaptioner, load_captioner

# æ–¹æ³•1: ä½¿ç”¨ä¾¿æ·å‡½æ•°ä¸€é”®åŠ è½½
captioner = load_captioner(checkpoint="outputs/ckpts/epoch19.pth")

# æ–¹æ³•2: æ‰‹åŠ¨åŠ è½½æ¨¡å‹
from models.vit_encoder_decoder import ImageCaptionModel
import torch

model = ImageCaptionModel(vocab_size=109)
model.load_state_dict(torch.load("outputs/ckpts/epoch19.pth"))
captioner = ImageCaptioner(model)

# å•å¼ å›¾ç‰‡æ¨ç†
caption = captioner.predict("path/to/image.jpg")
print(caption)  # "the sweater this lady wears has long sleeves ..."

# è¿”å› ID åºåˆ—
result = captioner.predict("image.jpg", return_ids=True)
print(result)  # {"text": "...", "ids": [3, 35, 30, ...]}

# æ‰¹é‡æ¨ç†
captions = captioner.predict_batch(["img1.jpg", "img2.jpg", "img3.jpg"])

# æ–‡ä»¶å¤¹æ¨ç†
results = captioner.predict_folder("path/to/folder", output_json="results.json")
```

### æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œ

```bash
# å•å¼ å›¾ç‰‡
python -m inference.infer \
    --checkpoint outputs/ckpts/epoch19.pth \
    --image path/to/image.jpg

# æ‰¹é‡æ¨ç†æ–‡ä»¶å¤¹
python -m inference.infer \
    --checkpoint outputs/ckpts/epoch19.pth \
    --folder path/to/images \
    --output results.json
```

---

## ğŸ”§ æ‰©å±•å¼€å‘

å¦‚æœä½ è¦æ·»åŠ æ–°çš„è®­ç»ƒä»»åŠ¡ï¼š

1. åœ¨ `trains/` ä¸‹åˆ›å»ºæ–°çš„è®­ç»ƒè„šæœ¬ (å¦‚ `task7.py`)
2. è®­ç»ƒå®Œæˆåå¾—åˆ°æƒé‡æ–‡ä»¶
3. ä½¿ç”¨è¯„æµ‹æ¨¡å—è¿›è¡Œç»Ÿä¸€è¯„æµ‹ï¼š

```python
from eval.evaluate import run_full_evaluation

results = run_full_evaluation(
    model=your_model,
    data_json="data/test.json",
    image_root="/your/image/path",
    experiment_name="task7_experiment",
)
```

---

## ğŸ“ License

MIT License


















